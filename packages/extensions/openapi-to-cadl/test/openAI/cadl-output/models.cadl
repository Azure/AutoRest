import "@cadl-lang/rest";

using Cadl.Rest;

namespace Azure.AI.OpenAI;

model paths·1vtxb06·deployments_deployment_id_completions·post·requestbody·content·application_json·schema {
  @doc("""
An optional prompt to complete from, encoded as a string, a list of strings, or
a list of token lists. Defaults to <|endoftext|>. The prompt to complete from.
If you would like to provide multiple prompts, use the POST variant of this
method. Note that <|endoftext|> is the document separator that the model sees
during training, so if a prompt is not specified the model will generate as if
from the beginning of a new document. Maximum allowed size of string list is
2048.
""")
  prompt?: post_content_schema_prompt;

  @doc("The maximum number of tokens to generate. Has minimum of 0.")
  max_tokens?: int32;

  @doc("""
What sampling temperature to use. Higher values means the model will take more
risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones
with a well-defined answer.
We generally recommend using this or `top_p` but
not both.
Minimum of 0 and maximum of 2 allowed.

""")
  temperature?: float32;

  @doc("""
An alternative to sampling with temperature, called nucleus sampling, where the
model considers the results of the tokens with top_p probability mass. So 0.1
means only the tokens comprising the top 10% probability mass are
considered.
We generally recommend using this or `temperature` but not
both.
Minimum of 0 and maximum of 1 allowed.

""")
  top_p?: float32;

  @doc("""
Defaults to null. Modify the likelihood of specified tokens appearing in the
completion. Accepts a json object that maps tokens (specified by their token ID
in the GPT tokenizer) to an associated bias value from -100 to 100. You can use
this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to
token IDs. Mathematically, the bias is added to the logits generated by the
model prior to sampling. The exact effect will vary per model, but values
between -1 and 1 should decrease or increase likelihood of selection; values
like -100 or 100 should result in a ban or exclusive selection of the relevant
token. As an example, you can pass {\"50256\" &#58; -100} to prevent the
<|endoftext|> token from being generated.
""")
  logit_bias?: unknown;

  @doc("The ID of the end-user, for use in tracking and rate-limiting.")
  user?: string;

  @doc("""
How many snippets to generate for each prompt. Minimum of 1 and maximum of 128
allowed.
""")
  n?: int32;

  @doc("""
Whether to enable streaming for this endpoint. If set, tokens will be sent as
server-sent events as they become available.
""")
  stream?: boolean;

  @doc("""
Include the log probabilities on the `logprobs` most likely tokens, as well the
chosen tokens. So for example, if `logprobs` is 10, the API will return a list
of the 10 most likely tokens. If `logprobs` is 0, only the chosen tokens will
have logprobs returned. Minimum of 0 and maximum of 100 allowed.
""")
  logprobs?: int32;

  @doc("The name of the model to use")
  "model"?: string;

  @doc("Echo back the prompt in addition to the completion")
  echo?: boolean;

  @doc("A sequence which indicates the end of the current document.")
  stop?: post_content_schema_stop;
  completion_config?: string;

  @doc("""
can be used to disable any server-side caching, 0=no cache, 1=prompt prefix
enabled, 2=full cache
""")
  cache_level?: int32;

  @doc("""
How much to penalize new tokens based on their existing frequency in the text
so far. Decreases the model's likelihood to repeat the same line verbatim. Has
minimum of -2 and maximum of 2.
""")
  presence_penalty?: float32;

  @doc("""
How much to penalize new tokens based on whether they appear in the text so
far. Increases the model's likelihood to talk about new topics.
""")
  frequency_penalty?: float32;

  @doc("""
How many generations to create server side, and display only the best. Will not
stream intermediate progress if best_of > 1. Has maximum value of 128.
""")
  best_of?: int32;
}

@doc("""
An optional prompt to complete from, encoded as a string, a list of strings, or
a list of token lists. Defaults to <|endoftext|>. The prompt to complete from.
If you would like to provide multiple prompts, use the POST variant of this
method. Note that <|endoftext|> is the document separator that the model sees
during training, so if a prompt is not specified the model will generate as if
from the beginning of a new document. Maximum allowed size of string list is
2048.
""")
model post_content_schema_prompt {}

@doc("A sequence which indicates the end of the current document.")
model post_content_schema_stop {}

model paths·maorw9·deployments_deployment_id_completions·post·responses·200·content·application_json·schema {
  id?: string;
  object?: string;
  created?: int32;
  "model"?: string;
  choices?: post_200_application_json_properties_itemsItem[];
}

model post_200_application_json_properties_itemsItem {
  text?: string;
  index?: int32;
  logprobs?: post_responses_200_content_application_json_schema_choices_item_logprobs;
  finish_reason?: string;
}

model post_responses_200_content_application_json_schema_choices_item_logprobs {
  tokens?: string[];
  token_logprobs?: float32[];
  top_logprobs?: Record<float32>[];
  text_offset?: int32[];
}

@error
model errorResponse {
  error?: errorResponse_error;
}

model errorResponse_error {
  code?: string;
  message?: string;
  param?: string;
  type?: string;
}

model paths·13piqoc·deployments_deployment_id_embeddings·post·requestbody·content·application_json·schema {
  @doc("""
An input to embed, encoded as a string, a list of strings, or a list of token
lists
""")
  input: post_content_schema_input;

  @doc("The ID of the end-user, for use in tracking and rate-limiting.")
  user?: string;

  @doc("input type of embedding search to use")
  input_type?: string;

  @doc("ID of the model to use")
  "model"?: string;
}

@doc("""
An input to embed, encoded as a string, a list of strings, or a list of token
lists
""")
model post_content_schema_input {}
